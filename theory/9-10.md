# Face Detection

La *pareidolia* è l’illusione subcosciente che tende a ricondurre a forme note, oggetti o profili (naturali o artificiali) dalla forma casuale. 

**Il volto è una biometria al limite, in quanto essa è soggetta a cambiamenti temporali. Il riconoscimento dell’iride è il sistema più affidabile, ma è anche quello più intrusivo. Le impronte digitali sono più facilmente accettate, ma non applicabili a soggetti non consenzienti.**

**Il volto possiede un’accettabilità molto elevata, mentre l’affidabilità deve essere ancora migliorata.**

Il volto è una **struttura tridimensionale piuttosto complessa** che quando viene acquisita, soprattutto se da un solo punto di vista, **diventa bidimensionale perdendo una parte delle informazioni**. Questa sua complessità è il limite che si ottiene quando si effettua l’acquisizione da un solo punto di vista, essa determina una certa inaffidabilità legata ad un altro limite sostanziale del volto e cioè la sua **eccessiva variabilità intra-classe**. Bisogna immaginare un aggiornamento periodico del template.

Un altro aspetto fortemente perturbativo dei sistemi di Face Detection/Recognition sono le **perturbazioni che possono essere indotte dall’acquisizione**; il volto è una delle poche biometriche che può essere variata dall’individuo a differenza di altre, infatti si possono produrre delle **variazioni volontarie sul volto** (espressioni o smorfie). Quando a queste variazioni si aggiungono ulteriori fattori perturbativi come la **variazione di posa e illuminazione**, si creano le *PIE variations* (Pose, Illumination, Expression).

L’altro fattore che non è indotto dalla persona e dalle condizioni ambientali è il cosiddetto Ageing (A-PIE, PIE variations + Ageing), ovvero la **variazione dell’età**. 

Quando si sviluppa una tecnica di face Recognition, essa la si testa su database comuni per comprendere, in modo comparativo, se l’idea algoritmica prodotta risulta essere migliore rispetto ad altre. Negli anni sono stati prodotti diversi dataset e, quando la comunità scientifica produce un nuovo algoritmo, tende ad utilizzare uno o più dataset per avere una misura comparativa con quello che è stato prodotto in precedenza.

## Approcci alla Face Detection

**Il primo passo è legato ad individuare all’interno di un video/immagine la presenza di un volto.** Una volta che il volto è stato individuato con qualche tecnica, allora è possibile darlo in input al sistema di classificazione che deve estrarre le caratteristiche principali. Il primo problema da risolvere è quello di costruire un face detector efficace, ovvero come capire se in un’immagine vi è un volto. 

La tecniche per effettuare operazioni di Face Detection sono diverse. Le più elementari, ma comunque estremamente performanti, sono quelle basate su **caratteristiche di basso livello** che possono riguardare alcune caratteristiche dei pixel (luminosità del volto o geometria del volto). Infatti, tutti i punti che individuano **occhi, narici, sopracciglia** ecc… creano **una costellazione di informazioni che possono essere legate da una serie di relazioni geometriche**. Oppure si può immaginare di avere un **template di un volto rappresentabile con un modello matematico**. Tutte queste sono caratteristiche di basso livello nelle quali si possono individuare le tecniche di face detection ***features-based***: esse sono tecniche che sfruttano alcune proprietà di basso livello del volto per individuarlo all’intero di una immagine.

Poi ci sono degli approcci che, piuttosto che basarsi sulle caratteristiche elementari del volto, sfruttano le immagini nella loro totalità, le cosiddette tecniche ***imaged-based***. Queste tecniche si basano sull’**addestramento di modelli computazionali**, in grado di riconoscere determinati oggetti sulla base di un pre-addestramento effettuato. Si costruisce un dataset di addestramento, dove all’interno ci sono sia immagini contenenti volti e sia immagini non contenenti volti, e si allena il sistema a riconoscere soltanto i volti.

**Una delle tecniche basate sulle caratteristiche di basso livello sfrutta la relazione tra il modello RGB e YCC.** Quest’ultimo è quello più utilizzato nell’image processing; è un modello nel quale è possibile separare una componente dell’immagine rispetto a delle altre, concentrando in questa componente di luminanza Y la maggior parte dell’informazione significativa. La transizione tra i due modelli avviene attraverso una trasformazione lineare che mappa il modello RGB nel modello YCC. Nel caso del modello YCC, l’informazione che interessa non è contenuta nella componente di luminanza Y, ma piuttosto nelle altre due componenti CC (crominanza). **Nella componente di crominanza sono particolarmente evidenti alcune caratteristiche del volto, in particolare occhi e bocca**; presa questa componente di crominanza, si applicano una serie di operatori (equalizzazione e AND) che tendono a massimizzare gli aspetti che interessano occhi e bocca. La loro individuazione permette di avere una certa probabilità che l’immagine in considerazione contenga un volto. Tale tecnica produce molti falsi positivi (volti dove non ci sono).

Un approccio che rientra più nell’ambito geometrico prevede l’individuazione della bocca e degli occhi ma vincola essi ad una certa relazione geometrica. Ovvero deve crearsi un triangolo che gode di alcune proprietà specifiche. Tale triangolo viene anche legato ad esser circoscritto da una circonferenza che rappresenta il volto vero e proprio. Si passa ad un approccio di natura geometrica in quanto ci si basa non solo sulla presenza di occhi e bocca ma anche sulle caratteristiche geometriche che vengono calcolate sulla base di una serie di operatori.

### Viola-Jones e Integral Images

Paul Viola e Michael Jones hanno proposto uno degli approcci di maggior successo (finora) alla localizzazione degli oggetti (incluso in *OpenCV*). L'algoritmo è **image-based** e può essere applicato al rilevamento del volto (ma anche al rilevamento di occhi e bocca in una strategia gerarchica). Esso richiede di **creare un classificatore inizialmente addestrato utilizzando più istanze della classe da identificare (esempi positivi), e diverse istanze di immagini che non contengono alcun oggetto della classe ma che possono causare un errore** (esempi negativi). Il training è progettato per estrarre diverse funzionalità dagli esempi e per selezionare quelle più discriminanti. 

Il modello statistico viene costruito in modo incrementale. *Misses* (un oggetto presente non viene rilevato) o *False Alarms* (un oggetto viene rilevato ma non è presente) possono essere **ridotti riqualificando l'aggiunta di nuovi esempi adatti** (positivi o negativi). L’algoritmo di Viola-Jones è un algoritmo composito in cui ci sono alcuni aspetti che di fatto mettono insieme un po' tutto quello che è stato analizzato in precedenza nell’ambito dell’elaborazione delle immagini. È un algoritmo molto veloce in quanto vengono posti in modo preliminare al processo di riconoscimento vero e proprio, **utilizzando dei classificatori, dei filtri e attraverso la combinazione di essi, mette a punto una strategia gerarchica** affinché riesca ad individuare se in un’immagine è presente un volto. 

L’algoritmo di Viola-Jones sfrutta le cosiddette **caratteristiche di Haar**, cioè delle caratteristiche tipiche del volto, condivise da tutti i volti umani. **Queste caratteristiche presenti nel volto vengono rappresentate da dei filtri che vengono sovrapposti all’immagine.** Tali filtri cercano di verificare la presenza di alcuni aspetti tipici del volto umano, come ad esempio la regione perioculare (occhi) è generalmente più scura della zona sottostante (zigomi) cosi come il ponte del naso è normalmente più luminoso degli occhi. Tendenzialmente, si possono utilizzare filtri molti simili a questi per cercare di capire se all’interno di una immagine c’è un’alternanza di regione scura/chiara, perché tale alternanza potrebbe essere sintomo di presenza di occhi e zigomi. Così come un’alternanza nero/bianco/nero potrebbe essere funzionale all’aver individuato occhio/ponte del naso/occhio. 

**È chiaro che queste sono condizioni necessarie ma non sufficienti per individuare un volto, ed è chiaro che di questi filtri se ne dovranno considerare diversi da far passare sul volto al fine di poter avere una certezza che quel che viene isolato sia effettivamente un volto.** Una volta individuato il filtro da far scorrere sull’immagine lo si sovrappone su di essa e si verifica la variazione fra zona chiara e zona scura: si moltiplicano per 1 i pixel dell’area bianca e per 0 i pixel dell’area nera e si effettua una differenza, che poi viene controllata se essa supera una certa soglia; se si supera una certa soglia allora vuol dire che quel filtro ha una forte probabilità di aver localizzato una regione dell’immagine. Il valore di una determinata funzione è sempre semplicemente la somma dei pixel all'interno di un rettangolo, meno la somma dei pixel all'esterno del rettangolo.

Viola e Jones hanno utilizzato le *funzioni a due rettangoli* (esistono funzioni anche a tre e quattro rettangoli). Ad esempio: la differenza di luminosità tra i rettangoli bianchi e neri su un'area specifica. Ogni funzione è correlata a una speciale posizione nella finestra secondaria. 

Una serie di filtri vengono applicati sull’immagine: diversi filtri dove ognuno dovrebbe individuare una parte significativa dell’immagine. Quando viene individuata un’area potenzialmente assimilabile ad un’immagine, il rettangolo si blocca in quanto si è in presenza del primo frame utilizzabile per classificare l’immagine come volto. I filtri possono avere diverse dimensioni e le finestre possono essere di diversa grandezza. 

Uno dei problemi di tale algoritmo è che dovrebbe unire efficacia ad efficienza, quindi  evidenziare i potenziali volti in un tempo piuttosto rapido. Il problema è che ogni qualvolta viene applicata una maschera di filtro, devono essere effettuate delle **semplici operazioni** (moltiplicazioni e sottrazioni) **ripetute un gran numero di volte su tutta l’immagine**, in quanto tali operazioni vengono effettuate per ognuno dei filtri con dimensione anche variabile dei filtri stessi. **Per ridurre il costo computazionale, l’immagine viene trasformata nella cosiddetta Integral Image.** 

**L’algoritmo definitivo utilizza delle Haar features in combinazione con una nuova rappresentazione dell'immagine detta Integral Image.** Le features hanno basso costo computazionale e la nuova struttura dati permette di effettuare l'analisi in tempo costante indipendentemente dalla dimensione delle regioni analizzate. **Viene introdotto un metodo di selezione di feature di Haar attraverso l'algoritmo AdaBoost di Freud Shapire** (1995). **Questa strategia permette di eliminare in addestramento la maggior parte delle feature di scarsa capacità discriminante e selezionare solo quelle più efficaci per il problema.** 

L’*immagine integrale* non è nient’altro che una rappresentazione/trasformazione più efficiente dell’immagine per realizzare queste moltiplicazioni e sottrazioni. Come in figura, l’immagine integrale viene costruita sommando i pixel in maniera progressiva. Quindi ogni pixel rappresenta la somma dei pixel precedenti in maniera crescente partendo da sinistra verso destra e dall’alto verso il basso. **La rappresentazione dell’immagine in termini di immagine integrale permette di realizzare la somma dei pixel in una qualsiasi area (rettangolo o quadrato di riferimento) attraverso un numero più efficiente di operazioni, ovvero solamente tre operazioni.** Indipendentemente dalla dimensione del filtro, si potrà sempre realizzare la somma dei pixel all’interno di un’immagine in termini di: A - B - C + D facendo fondamentalmente solo tre operazioni elementari; questo risulta essere un grande vantaggio in quanto applicando un filtro di Haar sull’immagine, si può avere in tempo constante (le sole tre operazioni) indipendentemente dal numero di pixel che sono coperti, la somma dei pixel. Poiché fondamentalmente nei filtri di Haar si deve effettuare la somma dei pixel per poi fare la differenza, la riuscita di queste operazioni velocemente è un grande vantaggio. La scelta dei filtri e il peso assegnato viene fatto con dei classificatori Ada Boosting. 

# Face Recognition

Una volta effettuata l’operazione di *detection*, ovvero una volta individuato il volto all’interno di un’immagine, si procede all’estrazione delle caratteristiche del volto che permettono di effettuarne il riconoscimento sulla base di un template, ovvero un modello corrispondente ad un individuo, affinché possa essere confrontato e verificato. La *recognition* avviene necessariamente dopo l'*enrollment*.

## Approcci alla Face Recognition

In letteratura esistono svariati approcci di tecniche di face Recognition.

### ICA & PCA

Tra le tecniche più utilizzate soprattutto in passato per il Face Recognition vi è la PCA (*Principal Component Analysis*) che rientra nella categoria delle *Image-Based*. Tale tecnica, sotto alcuni aspetti, ricorda la trasformata di Fourier, ma con un approccio totalmente diverso. **Piuttosto che considerare i pixel, essi vengono associati a delle variabili geometriche**, ossia vettori in cui è massima la variazione tra i vettori stessi. **L’obiettivo principale della PCA è di individuare dei vettori che siano efficienti per rappresentare determinati dati.** 

Si assuma di avere delle immagini quadrate di dimensione `N`, in cui il numero di pixel risulta essere `N*N`. Con `M` il numero di immagini del database e `P` il numero di persone coinvolte all’interno del database. 

**L’algoritmo della PCA non fa nient’altro che cercare di individuare all’interno di un insieme di immagini, le immagini più significative; quindi tentare di trovare un numero di immagini molto più piccolo di `M` tale che queste immagini siano rappresentative di tutto il dataset**. Quindi trovare una strategia che, in presenza di un dataset molto corposo, possa dare in output un insieme molto più piccolo di immagini significative. Tali immagini significative possono rappresentare tutto il dataset; riassumendo, **si tenta di ridurre il problema di rappresentazione ad un problema più piccolo trasformando la *dimensionalità* del problema.** 

L’idea è quella di rappresentare ogni volto attraverso la combinazione lineare dei volti più significativi, i cosiddetti *Eigenfaces*. Quindi ogni volto significativo contribuirà alla rappresentazione di un volto all’interno dell’immagine.

#### Eigenfaces

**Si prendono in input le immagini e per ognuna di esse si effettua una linearizzazione, cioè ogni immagine viene rappresentata come vettore. Poi, si calcola la media su tutti i vettori e tale media viene sottratta da ciascun vettore.** Quest’ultima operazione fa in modo che **venga rimossa la parte in comune tra tutti i volti**, ottenendo quindi le immagini che andranno a costituire la base per la *matrice di covarianza*; si scelgono le immagini meno correlate tra di loro (quelle più vicine ad essere linearmente indipendenti). Una volta individuato l’insieme dei vetri (immagini) meno correlate, è possibile **rappresentare tutta la base di dati attraverso una combinazione lineare degli Eigenfaces**. Quindi la scelta delle immagini più indipendenti e più significative permette la generazione di tutta quanta la base di dati. Quindi, se il database possiede `M` immagini, si cercano `k` immagini con `k` molto piccolo, tale da poter rappresentare tutte le immagini del database attraverso la combinazione lineare delle `k` scelte. Esse vengono scelte grazie alla matrice di covarianza che permette di stabilire quali immagini risultano essere meno correlate tra esse; meno le immagini sono correlate, più sono tra di loro indipendenti.

### Neural Networks

Una rete neurale simula il funzionamento dei neuroni nel cervello. Ciascun neurone è rappresentato da una funzione matematica basata sul calcolo delle probabilità. **Per il riconoscimento dei volti, l’ottimo sarebbe utilizzare un neurone per ciascun pixel.** Questo approccio richiede troppi neuroni. La soluzione consiste nell’utilizzare una rete di neuroni per «riassumere» l’immagine in un vettore più piccolo, mentre una seconda rete effettua il riconoscimento vero e proprio.

### Graph-Based Systems

Attraverso filtri e funzioni di localizzazione vengono localizzati sul volto un insieme di punti di riferimento. Questi punti vengono collegati da archi pesati e si ottiene un grafo. Ad ogni volto è associato un grafo, per cui confrontare due volti significa confrontare due grafi.

### Termogramma

L’immagine del volto viene acquisita mediante un sensore termico. Il sensore rileva le variazione di temperatura dell’epidermide del volto. L’immagine viene segmentata e indicizzata.