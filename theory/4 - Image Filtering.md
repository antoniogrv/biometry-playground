 # Image Filtering
 
 In generale una trasformazione dell’immagine è caratterizzata nel seguente modo: g(x,y) = T[f(x,y)] dove: - f è l’immagine input 
 - g è l’immagine elaborata 
 - T è l’operatore di trasformazione applicata ai singoli pixel dell’immagine. 

L’operatore che si applica all’immagine può avere diverse caratteristiche matematiche. 
Si possono definire degli operatori lineari se godono di determinate proprietà come: 
- Omogeneo: T[αf(x,y)] = αT[f(x,y)] 
- Associativo: T[f(x,y) + g(x,y)] = T[f(x,y)] + T[g(x,y)]

Il fatto che un operatore possa essere lineare o meno non ha un impatto sulla tipologia di trasformazione che si va ad applicare. Innanzitutto, va fatta una classificazione delle trasformazioni T che possono essere applicate ad una determinata immagine/video: 
- Tecniche operanti nel dominio spaziale, ovvero dove vi è una manipolazione diretta dei pixel. 
- Tecniche operanti nel dominio delle frequenze (trasformata di Fourier), ovvero vi è una trasformazione dell’immagine nel dominio delle frequenze e conseguente manipolazione di esse. Tali frequenze non agiscono sui pixel ma agiscono su una trasformazione dei pixel su un concetto diverso. Come si può intuire, tali tecniche implicano un costo computazionale aggiuntivo compensato da una qualità della trasformazione dell’immagine migliore. 
- Tecniche ibride, ovvero una combinazione di varie tecniche.

L’altro elemento che si può aggiungere nell’ottica della classificazione delle tecniche, è quello che permette di dividere le tecniche di trasformazione in 4 famiglie. 
- Point Operations: le tecniche che rientrano in questa classe risultano essere molto semplici, ovvero trasformazioni dove il pixel di destinazione, nella trasformazione, dipende solo dal pixel di partenza. 
- Local Operations: hanno una struttura più aggregata, dove il pixel di destinazione non dipende solo da un pixel di partenza, ma anche da un suo intorno quadrato. 
- Object Operations: il pixel di destinazione dipende dal pixel di partenza e da un oggetto che ha un suo significato semantico, non di forma regolare come nel caso della Local Operations. 
- Global Operations: ogni pixel dell’immagine di destinazione, dipende da tutti i pixel dell’immagine di partenza. 

# Filtri e trasformazioni

Si analizza ora qualche semplice trasformazione: partendo con il Point Processing, dove ogni pixel di destinazione dipende solo dal corrispettivo pixel di partenza. In questa rappresentazione grafica, sull’asse delle ascisse si hanno gli input gray level (livelli di grigio dell’immagine di partenza), r invece rappresenta il valore output e T effettua la trasformazione. Per ogni pixel situato su s, T ne ottiene una trasformazione che lo porta ad assumere un nuovo valore lungo la retta T(s). Questa trasformazione che si sta effettuando, T(s), è una trasformazione monotona decrescente, che prende il nome di Negativo. È possibile appore anche altre operazioni come la Binarizzazione (T è monotona crescente) di una immagine. In essa i valori ottenuti in fase di digitalizzazione, mappati sull’asse delle ascisse, sono valori che devono essere distribuiti soltanto su due colori, ovvero il bianco e il nero. Ottenendo un’immagine di output, appunto, bianco e nera. Oltre alla binarizzazione, agendo sui due cerchi (a destra) è possibile anche effettuare il Contrast Stretching dell’immagine, operando direttamente sull’istogramma per migliorare il contrasto dell’immagine in questione.

# Binarizzazione:

Questa trasformazione individua una soglia in 110 (variabile), dove tutti i colori da 0 fino a 110 diventeranno nero, e tutti i colori che partono da 110 fino a 255 diventeranno bianco. Ottenendo la stessa immagine di partenza, ma binarizzata. 

# Negativo

Vi è un'immagine col proprio instagramma. Produrre il negativo di tale immagine, significa associare ad ogni pixel il livello di grigio complementare. Infatti, tra le due immagini intercorre una differenza specifica: i pixel chiari sono diventati più scuri e viceversa.

Andando avanti ci si concentra sulle Local Operations, dove la trasformazione T è definita in un intorno predefinito di (x, y). Con i filtri spaziali (mask processing) è possibile effettuare varie operazioni. Queste trasformazioni sono definite dei filtri spaziali. Si possono individuare tre categorie di filtri che si andranno ad applicare alle immagini: • Lowpass (passa basso): attenua o elimina le alte frequenze, ovvero contorni e dettagli. • Highpass (passa alto): attenua o elemina le basse frequenze, come contrasto e intensità. • Bandpass (passa banda): attenua frequenze in una banda predefinita. Filtri Spaziali Questi filtri rientrano nella categoria dei Local Operations dove ogni pixel di destinazione dipende dal pixel di partenza più il suo intorno. Per essere costruiti, in genere vengono create delle mascherine quadrate, generalmente di dimensioni dispari 3x3, 5x5 ecc… e dentro tali mascherine verranno posti dei pesi, che chiaramente andranno a determinare la funzionalità del filtro. Sulla base dei valori dei valori W1 … W9, distribuiti all’interno della mascherina si otterrà un risultato diverso. Una volta costruita, essa si sovrapporrà all’immagine centrandola nel pixel (x, y) che avrà come corrispondete il peso W5 e si effettuerà la moltiplicazione tra ciascuno di questi pesi con il corrispondente valore di luminosità presente nell’immagine.

Questa prima classe di filtri verrà utilizzata per costruire i cosiddetti Smoothing Filter, cioè i filtri utilizzati nelle operazioni di preprocessing. Infatti, può capitare che l’immagine contenga del “rumore” oppure sia troppo ricca di dettagli. Con tali filtri è possibile intervenire, anche se essi possono produrre un effetto di Blurring se l’immagine è troppo ricca di dettagli, e quindi se ne vogliono rimuovere alcuni di essi per rendere l’immagine più funzionale per l’estrazione di oggetti. Oppure l’immagine presenta del rumore che può esser stato introdotto o nella fase di digitalizzazione oppure durante la fase di trasmissione. Questi filtri ricadono nella categoria dei filtri Lowpass (bassa passo) ovvero i filtri che lasciano transitare solo le informazioni che risiedono nelle basse frequenze come contrasto e intensità, i filtri Lowpass si dividono in lineari e non lineari. Il primo filtro che verrà affrontato è il Filtro Media (aritmetica) il quale fa parte dei filtri lineari. I filtri non lineari invece, sono il Filtro Mediana, Filtro Max e Filtro Min.

# Filtri Spaziali

In sintesi, la Media presenta una eccessiva velatura (Blurring) al crescere della maschera e delle iterazioni con conseguente perdita dei dettagli. Essa però preserva con la versione adattive le forme (scalini). La Mediana ha una riduzione del rumore (noise reduction) senza eccessiva perdita di dettagli (Blurring limitato). Non preserva le forme (scalini)

## Media

Tale filtro non differisce dal concetto di media aritmetica. Quindi ogni pixel dell’immagine destinazione viene sostituito con la media dei pixel appartenenti ad un’area ben definita dell’immagine. Piuttosto che prendere soltanto il pixel (x, y) nell’immagine input, viene preso la media di tutti i pixel connessi con il pixel (x, y). Effettuando la media (con la divisione per 9) ottenendo il filtro di destinazione. 

L’applicazione di un filtro media ad un’immagine/video che presenta rumore, si veda l’esempio seguente, viene effettuato proprio per eliminare tale rumore prodotto da un determinato motivo, per ricondurre l’immagine ad una qualità migliore. Il contorno di un oggetto è una transizione, tra un oggetto e un altro, come una sorta di interruzione.

## Media Adattiva

È possibile modificare il filtro media per ottenere un risultato qualitativamente migliore. La media adattiva va a risolvere varie problematiche della media, come la confusione dei contorni. Ogni qual volta essa viene utilizzata effettua un confronto tra il pixel (x, y) e tutti i quanti i pixel che sono nel suo intorno. In presenza di piccole variazioni si applica la media, ma in presenza di grandi variazioni si lasciano invariati i pixel di riferimento. Essa lavora sui bordi. Applicando un filtro media sempre più grande, l’immagine tende a sfocarsi e perdere rumore ma anche dettagli. Quindi la media adattiva può essere una soluzione che consente di preservare in parte dettagli e i contorni degli oggetti avendo un buon effetto sulla rimozione del rumore. 

## Mediana 

Un’altra possibile soluzione che può essere utilizzata per rimuovere il rumore è il Filtro Mediana. Esso corrisponde esattamente al concetto di mediana statistica per il quale si considerano i pixel di una regione, li si ordina in maniera crescente e poi ci si posiziona sul valore centrale. La mediana è proprio tale valore centrale. Rispetto alla media, la mediana non viene influenzata dalle code, ovvero il valore centrale non viene condizionato particolarmente dalla presenza di significativi cambiamenti agli estremi, cosa che avviene nella media.

# Smoothing Gaussiano

Il funzionamento del Filtro Gaussiano è simile a quello del filtro mediana con la differenza che il contributo di ciascun pixel limitrofo ha un peso diverso, definito dalla distribuzione spaziale della gaussiana. Nel campo dell’elaborazione delle immagini la gaussiana viene troncata limitandone l’estensione ad una zona di dimensioni N * N (i valori più frequenti per N sono 3, 4 e 5). Si parte dal presupposto che i pixel in prossimità della zona centrale, sono quelli più importanti. Quindi se si vuol posizionare la maschera su un pixel (x, y), non si deve assegnare poi lo stesso peso a tutti gli altri pixel ma si cerca di concentrare la maggior parte del peso in prossimità del pixel (x, y), e progressivamente diminuire il peso dei pixel man mano che ci si allontana dal centro. La somma di tutti i pesi sarà sempre uguale a uno. In prossimità del pixel di riferimento, si assegnerà un valore maggiore. Quindi possono essere costruiti nuovi filtri che, fondamentalmente, sono dei Filtri Media pesati. 

Mentre prima il peso assegnato dalla media era uguale per tutti quanti, ora si adopera una selezione: più ci si è vicini alla posizione (x, y) maggiore sarà il peso in quanto maggiore sarà la probabilità che quel pixel appartenga alla regione di interesse; più ci si allontana dal pixel (x, y) maggiore sarà la probabilità di non appartenere alla regione di interesse e quindi si cerca di contaminare il meno possibile la trasformazione. Con un Filtro gaussiano si riesce ad eliminare le zone di rumore e a preservare gli scalini, proprio perché in prossimità di essi i pixel otterranno una distribuzione di pesi differenziata.

# SUSAN: Smoothing Segment Univalue Assimiliating Values

Tale filtro esaspera il concetto Gaussiano. Infatti, permane l’idea di Filtro Gaussiano dove però i pesi non vengono stabiliti a priori come fatto in precedenza, ma in questo caso i pesi vengono calcolati run-time. Quindi viene applicata la maschera all’immagine e si va a controllare quali pixel ricadono nella stessa area del pixel centrale. Si assegneranno pesi alti a tutti i pixel che appartengono all’aerea del pixel centrale e si tenderà a dare peso minore ai pixel che si trovano più in lontananza rispetto al centro. Il filtro SUSAN preserva gli scalini, risultando essere un filtro ottimale fornendo output migliori.

# Edge Detection 

Si introduce un’altra classe di filtri che hanno come obiettivo quello di trattenere le informazioni che risiedono nelle alte frequenze, lavorando in modo esattamente speculare a quelli precedenti. Tali filtri quindi trattengono i punti di controllo, ovvero i dettagli. Gli Edge Detector sono filtri che hanno la capacità di individuare le aree di transizione tra regioni omogenee: 
- Punti isolati.
- Linee (orizzontali, verticali, oblique). 
- Contorni (insieme di punti con inizio e fine coincidenti). 

Come detto, gli edge possono avere forme differenziate; tendenzialmente si identificano come una variazione di una serie di pixel o anche un singolo pixel, che ha una caratteristica ben definita: ovvero si passa da un’area a transizione costante (vicina alla 0) ad un’area in cui c’è una variazione (o perturbazione). Questa variazione può avere varie forme: rampa, scalino, ecc… Tra le possibili situazioni di transizione tipiche, vi sono quelle a rampa e scalino. Il contorno a scalino presenta una transizione brusca tra due zone uniformi, invece nella transizione a rampa il passaggio tra le due regioni risulta essere più attenuato. 

L’obiettivo è quello di individuare una sorta di detector che vada a mettere in evidenza le cime più alte delle variazioni di luminosità. La linea in rosso ha il compito di individuare quelle transizioni che hanno più alta probabilità di essere dei punti di contorno, non delle semplici variazioni all’interno dell’oggetto ma delle variazioni significative che indicano una transazione da un oggetto ad un altro. Si supponga di avere una rappresentazione di un’immagine con una certa variabilità di toni di grigio. Sull’asse dell’ascisse vi sono i pixel e sull’asse delle ordinate i valori di grigio. Se viene calcolato la derivata prima della funzione in questione, si avrà come risultato delle informazioni sulla presenza di variazioni; quindi la derivata prima risulta essere uguale a zero quando non vi sono valori significativi e risulta avere una variazione laddove esiste una variazione dei livelli. 

Ritorna ad esser costante in presenza della zona uniforme, per poi trovare un’altra variazione e terminare in modo uniforme. Quello che si vuol fare è capire se questi punti di massimo e minimo individuati dalla derivata prima, possono essere significativi. Quindi si vuol individuare una soglia che può dare informazioni circa i punti individuati al fine di comprendere se possono essere variazioni utili ad individuare un passaggio da un’area uniforme ad un’altra. Questi pixel presi singolarmente o in modo aggregato, sono quelli che possono essere potenzialmente etichettati come pixel di transizione ovvero Edge Pixels. La difficoltà fondamentale è capire la soglia utile, e se essa può essere resa universale oppure deve essere adattata a classi di immagini, e se viene individuata per classi di immagini si può esser sicuri che questa soglia vada bene per tutta quanta l’immagine. Ovviamente NO in quanto la soglia è molto difficile da individuare e non sempre quella individuata per classi di immagini vada bene. 

Spesso tale soglia all’interno della stessa immagine permette sia di individuare pixel significativi ma individua potenziali pixel di contorno che non risultano esserlo. Questo pone un limite dell’applicazione di tali filtri alle immagini digitali. 

## Punti Isolati

I punti isolati sono le discontinuità più semplici da individuare, in quanto non contribuiscono a definire la morfologia di un oggetto. Il punto corrispondente alla posizione centrale della maschera può essere considerato un punto isolato quando, per esempio con riferimento alla maschera mostrata si ha: R > T, essendo T un opportuno valore di soglia (non negativo). Anche in questo caso, quando si individua un punto isolato affinché tale punto possa essere etichettato come punto di discontinuità, deve esserci una soglia T che viene individuata. 

Tale soglia T deve far sì che una volta applicata la maschera ad una determinata area ed una volta calcolato il valore R, si è in grado di etichettare il pixel a cui corrisponde R come pixel di edge, soltanto se il valore è abbastanza significativo. Quindi il valore di grigio di quel punto isolato deve essere abbastanza differente da quello dei suoi vicini. Se non vi è tale differenza evidente, non si può etichettare questo come punto isolato. In figura un esempio, in cui vi è una prima possibile rappresentazione di un filtro di Edge Detector basato sulla derivata prima. La maschera 3x3 presenta valori negativi e un valore positivo centrale, e la somma di tutti valori corrisponde a 0. 

Quindi in linea col concetto di derivata prima, questo filtro deve garantire che quando viene applicato all’immagine si dovrà avere che nelle zone uniformi, il risultato deve essere zero, quindi il pixel kernel non è un pixel candidato ad essere. un pixel di contorno, quando invece si trovano delle variazioni, esse devono essere evidenziate. Il principio portante di tali maschere, come detto, è che la somma dei pesi deve essere uguale a zero, per assicurare il concetto di rapporto incrementale. Quest’ultimo sarà nullo nelle aree in cui non c’è variazione, e ci sarà un rapporto incrementale evidente positivo o negativo quando esiste una variazione. 

## Linee

È possibile costruire dei filtri che sono capaci di individuare non soltanto i punti, ma anche le linee, quindi privilegiare quei pixel che sono presenti lungo linee uniformi di variazione. Come il filtro precedente, hanno la caratteristica di avere pesi negativi e positivi, e la somma di tuti i pesi è uguale a zero. Ognuno dei filtri riesce ad individuare una particolare caratteristica di linea. Il primo individua punti di contorno che hanno uno sviluppo orizzontale, il secondo invece punti di contorno che hanno uno sviluppo prevalentemente obliquo, il terzo punti di contorno che hanno uno sviluppo prevalentemente verticale, ed infine il quarto individua punti di contorno che hanno uno sviluppo obliquo ma con un senso rispetto al secondo. La prima maschera risponde più intensamente alle linee orizzontali (spesse un pixel): con uno sfondo costante, il massimo di R si ha infatti quando la linea coincide con la riga centrale della maschera. 

Analogamente, la seconda maschera risponde meglio a linee orientate a 45°, la terza a linee verticali e la quarta a linee orientate a 45°. Si supponga di passare tutte e quattro le maschere su una immagine, e che in un certo punto si avranno R1, R2, R3, R4 risposte, diverse, delle quattro maschere. Per capire se il pixel appartiene potenzialmente ad una maschera bisogna verificare quale valore Ri risulta essere più grande rispetto agli altri; una volta individuato tale valore, allora il punto di edge che si sta etichettando è associato alla maschera Ri e si trova lungo una linea specifica di quella determinata maschera. 

## Contorni

L’estrazione dei contorni (edge) è sicuramente uno degli argomenti che hanno ricevuto più attenzione nella letteratura sull’image processing. Il contorno di un oggetto rappresenta infatti la separazione tra l’oggetto e lo sfondo o tra l’oggetto ed altri oggetti, per cui la sua estrazione è molto spesso il primo passo verso l’individuazione dell’oggetto. Un edge si presenta in una immagine come il confine tra due regioni caratterizzate da proprietà dei livelli di grigio in qualche modo distinguibili. Nel seguito si ipotizzerà che le regioni in questione siano sufficientemente omogenee, di modo che la determinazione della transizione tra le due regioni sia possibile sulla sola base della discontinuità dei valori di grigio. Le prime tecniche di edge detection che verranno analizzate sono basate sull’applicazione di un operatore locale di derivata. 

Per rendere le regioni sufficientemente omogenee tra loro è possibile applicare uno Smoothing in modo che vengano individuate soltanto le variazioni importanti. Per costruire dei filtri di edge detector basati sul principio di derivata è molto semplice, rispettando poche regole. Infatti, si prende una mascherina di dimensione dispari 3x3, 5x5, ecc… si assegnano i pesi in modo tale che essi siano tra di loro bilanciati con valori positivi e negativi e assicurando che la somma sia uguale a zero. Uno dei problemi che si presenta è quello di stabilire se è necessario introdurre una soglia. L’introduzione di una soglia è una delle problematiche più importanti nelle fasi di costruzione degli edge detector. 

Nella figura vi è una immagine qualitativamente scadente, ed effettuare un’operazione di edge detector su questa immagine è piuttosto difficile estraendo il profilo degli oggetti che sono presenti. La forma del pedone bianco è facilmente ottenibile siccome essa riesce a distinguersi all’interno della scacchiera. Il pedone nero invece, si confonde con il tassello nero alle sue spalle. Quindi la determina della soglia che permette di tirar fuori il profilo dell’oggetto è abbastanza complicata. I progettisti di edge detector hanno a disposizione questa soglia e possono decidere se utilizzare una soglia alta o bassa e questo produce degli effetti differenti nella qualità dell’immagine. Un edge detector trasforma un’immagine in una immagine bianco e nera quindi con solo due livelli di grigio, in quanto un edge detector deve solo stabilire se ogni pixel di partenza è un pixel di contorno oppure no. Se un pixel non è di contorno viene etichettato come nero (valore 0), se un pixel è di contorno viene etichettato come bianco (valore 255). 

La scelta della Threshold (soglia) è fondamentale per stabilire quali sono i pixel tendenzialmente candidabili a diventare pixel di contorno; quindi una volta stabilita la soglia, tutti i pixel che si trovano al di sopra della soglia sono pixel di contorno, viceversa se si trovano sotto tale soglia non sono pixel di contorno. Infine, c’è una fascia grigia, che potrebbe oscillare tra una soglia troppo bassa o troppo alta. 
- Con Low Threshold si individuano potenziali pixel di contorno anche pixel che hanno variazioni piuttosto piccole. Tali variazioni possono introdurre come potenziali pixel di controllo anche pixel che non lo sono. Questo tipo di soglia permette di individuare il profilo del pedone nero. 
- Con High Threshold si corre il rischio di ridurre troppo il numero di pixel che vengono etichettati come pixel di contorno, dando come risultato finale una quasi scomparsa totale della morfologia dell’oggetto. Il pedone ha perso tutta la parte centrale. 

Il fatto che la derivata prima e la derivata seconda del profilo siano significativamente diverse da 0 soltanto in corrispondenza alle transizioni costituisce la motivazione dell’uso di operatori derivativi per l’estrazione dei contorni. La derivata prima del profilo è positiva in corrispondenza di una transizione scuro-chiaro, negativa in corrispondenza di una transizione chiaro-scuro, nulla nelle zona a livello di grigio costante. 

La derivata seconda è positiva in prossimità di un contorno, dalla parte scura del contorno stesso, negativa dalla parte chiara del contorno, nulla nelle zone a livello di grigio costante, ed esibisce un passaggio per lo zero o zero crossing esattamente in corrispondenza delle transizioni. Ovvero, la derivata prima indica la presenza di un massimo, la derivata seconda fornisce l’esatta posizione del massimo, nella posizione in cui il rapporto incrementale smette di crescere per iniziare la decrescita. 

Riassumendo, il valore della derivata prima può essere utilizzato per determinare la presenza di contorni in una immagine. Gli zero crossing della derivata seconda ne possono consentire la precisa localizzazione. Il segno della derivata seconda permette di stabilire l’appartenenza di un pixel al versante scuro o al versante chiaro di un contorno. L’applicazione dei concetti precedentemente illustrati necessita tuttavia di alcune cautele, essenzialmente legate alla natura digitale delle immagini.

## Gradiente

Il gradiente rappresenta uno degli strumenti attraverso il quale si può cercare di individuare le zone di transizione. Il gradiente di una funzione a due variabili viene calcolato come in figura. Del gradiente è possibile calcolare due informazioni: l’entità del gradiente (la grandezza della variazione) e la direzione del gradiente, ovvero in che direzione si muove il contorno. La direzione dell’edge è quella perpendicolare rispetto al gradiente. Il gradiente è uno strumento sofisticato che permette di calcolare due informazioni: se un pixel è etichettato come pixel di contorno sulla base della dimensione del gradiente, infatti più è grande la variazione, più il pixel ha probabilità elevate di essere etichettato come pixel di contorno. Il secondo aspetto è legato alla direzione del contorno che viene calcolata come perpendicolare rispetto alla direzione del gradiente. 

## Laplaciano

Il Laplaciano è una rappresentazione della derivata seconda, che a differenza del gradiente, è caratterizzato dall’avere un pixel centrale positivo, le direzioni principali con valori negativi, e gli altri posti uguali a zero. Si prendono i coefficienti della derivata seconda della funzione (x, y) rispetto a x, a due variabili. E questi poi costituiscono poi i coefficienti che si andranno a riprodurre all’interno della matrice. Questa rappresentazione del Laplaciano è la rappresentazione discreta di una derivata seconda.

## Canny Edge Detector

Uno degli operatori maggiormente utilizzati per estrarre i contorni è l’operatore edge detector Canny, il quale si è dimostrato molto efficace. Questo operatore mette insieme tutto ciò che è stato detto finora, e sperimentalmente si è visto che è in grado di produrre risultati molto significativi. Questo perché il metodo di Canny ha un’elevata possibilità di produrre edge connessi, cioè contorni chiusi o piuttosto continuativi che sono uno degli obiettivi che si ricercano all’interno delle operazioni di edge detection. Quando si vuole effettuare l’estrazione di contorni e preservare la chiusura di essi, Canny fornisce questo tipo di prestazioni. L’algoritmo di Canny prevede: 1. Smoothing gaussiano dell’immagine, per eliminare le piccole transazioni. 2. Calcolo del gradiente. 3. Soppressione dei non-massimi in direzione ortogonale all’edge. 4. Selezione degli edge significativi mediante isteresi. La qualità del risultato dell’operatore Canny dipende da: • Ampiezza della gaussiana nella prima fase. • Dimensione del filtro nella prima fase. • T1 e T2, soglie per l’isteresi nell’ultima fase. 

# Sharpening 

Un’operazione di Sharpening tende ad evidenziare i dettagli fini, attraverso un’operazione che prevede prima l’estrazione dei contorni e poi la sovrapposizione dei contorni con l’immagine originale. Quindi non si ottiene una immagine in bianco e nero ma, con le operazioni di Sharpening, dopo aver estratto i contorni essi vengono sovrapposti all’immagine, per renderli ancor più evidenti. 